{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is divided into three parts:\n",
    "1. The speech-to-text to get the user's input\n",
    "2. The LLM to generate a response\n",
    "3. The text-to-voice to output the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Speech recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll need two functions, one which converts an audio file to text andd one to get voice from the mic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "r = sr.Recognizer()\n",
    "\n",
    "def recognize_speech_from_mic(recognizer):\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        r.pause_threshold = 2\n",
    "        audio_text = r.listen(source)\n",
    "    print(\"Recognizing...\")\n",
    "        \n",
    "    try:\n",
    "        return r.recognize_google(audio_text)\n",
    "    except:\n",
    "        print(\"Could not recognize ...\")\n",
    "        return recognize_speech_from_mic(recognizer)\n",
    "\n",
    "# print(recognize_speech_from_mic(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll need a function to keep a conversation with an llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def groq_chatbot_conversation(new_message, model=\"llama-3.1-8b-instant\", api_key=None, history_file=\"conversation_history.txt\"):\n",
    "    \"\"\"\n",
    "    A function to interact with a Groq-based chatbot that maintains conversation context using a text file.\n",
    "\n",
    "    Parameters:\n",
    "    - new_message (str): The user's latest input message.\n",
    "    - model (str): The model to use (default: \"llama-3.1-8b-instant\").\n",
    "    - api_key (str): Groq API key (default: None, will use GROQ_API_KEY from environment variables if not provided).\n",
    "    - history_file (str): The file to store conversation history (default: \"conversation_history.txt\").\n",
    "\n",
    "    Returns:\n",
    "    - str: The chatbot's response.\n",
    "    \"\"\"\n",
    "    api_key = api_key or os.getenv(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"API key is required. Set it via the 'api_key' parameter or the 'GROQ_API_KEY' environment variable.\")\n",
    "    \n",
    "    # Load conversation history\n",
    "    if os.path.exists(history_file):\n",
    "        with open(history_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            messages = json.load(file)\n",
    "    else:\n",
    "        messages = []\n",
    "    \n",
    "    # Append new user message\n",
    "    messages.append({\"role\": \"user\", \"content\": new_message})\n",
    "    \n",
    "    client = Groq()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=1,\n",
    "        max_completion_tokens=1024,\n",
    "        top_p=1,\n",
    "        stream=True,\n",
    "        stop=None,\n",
    "    )\n",
    "    \n",
    "    response_text = \"\"\n",
    "    for chunk in completion:\n",
    "        response_text += chunk.choices[0].delta.content or \"\"\n",
    "    \n",
    "    # Append assistant response\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "    \n",
    "    # Save updated conversation history\n",
    "    with open(history_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(messages, file, indent=4)\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "# Example usage:\n",
    "# print(groq_chatbot_conversation(\"It's me!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was close, but I clearly misremembered the lyrics. Thank you for correcting me and being kind about it. \"Hello\" is a classic Adele song, but I guess I need to brush up on my lyrics. If you'd like to share the correct lyrics, I'd be happy to learn from my mistake!\n"
     ]
    }
   ],
   "source": [
    "print(groq_chatbot_conversation(\"the song name is correct but you kinda wiffed on the lyrics ngl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text-to-Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we'll need a function that converts text to speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import edge_tts\n",
    "import io\n",
    "import nest_asyncio\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "# Allow asyncio to work in Jupyter Notebook\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def speak(text: str, voice=\"en-US-AriaNeural\", rate=\"+100%\"):\n",
    "    \"\"\"Convert text to speech and play it directly in Jupyter Notebook.\"\"\"\n",
    "    tts = edge_tts.Communicate(text, voice,rate =rate)\n",
    "    stream = io.BytesIO()\n",
    "\n",
    "    async for chunk in tts.stream():\n",
    "        if chunk[\"type\"] == \"audio\":\n",
    "            stream.write(chunk[\"data\"])\n",
    "\n",
    "    stream.seek(0)\n",
    "    audio = AudioSegment.from_file(stream, format=\"mp3\")\n",
    "    play(audio)\n",
    "\n",
    "# Function to run async code in Jupyter\n",
    "async def tts_play(text, voice=\"en-US-AriaNeural\", rate=\"+100%\"):\n",
    "    await speak(text, voice, rate)\n",
    "\n",
    "# Example usage\n",
    "# await tts_play(\"\"\"It was a dark and stormy night. All of a sudden, a voice came out of the darkness and said, \"Hello! I'm here to help you with your query. How can I assist you today?\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part will use the functions from the other parts to create a loop containing all the voice bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "Recognizing...\n",
      "user:  well it's more like another name that's going to take as input my voice and then three kids and the response to it through the other room and then get through a text to speech module to answer me\n",
      "bot:  So it sounds like you're trying to create a voice-controlled system, where you speak to an assistant, and it responds back to you through a text-to-speech module.\n",
      "\n",
      "You're using a framework that can process your voice input, understand your requests, and then send responses back to you through a text-to-speech module. This is often referred to as a voice assistant or a voice interaction system.\n",
      "\n",
      "In this case, the hyperparameter tuning you're doing is probably related to the speech recognition module, which needs to accurately transcribe your voice input into text. Tweaking the speaking rates, pitch, and cadence might be related to the text-to-speech module, which needs to convert the text response into human-like speech that sounds natural and clear.\n",
      "\n",
      "Is that correct? And are you using a specific platform or framework for this, such as Dialogflow, Microsoft Bot Framework, or Amazon Alexa Skills Kit?\n",
      "Listening...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno -9988] Stream closed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m, in \u001b[0;36mrecognize_speech_from_mic\u001b[1;34m(recognizer)\u001b[0m\n\u001b[0;32m      8\u001b[0m     r\u001b[38;5;241m.\u001b[39mpause_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 9\u001b[0m     audio_text \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecognizing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ben-g\\code\\friend\\venv\\Lib\\site-packages\\speech_recognition\\__init__.py:460\u001b[0m, in \u001b[0;36mRecognizer.listen\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration, stream)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 460\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ben-g\\code\\friend\\venv\\Lib\\site-packages\\speech_recognition\\__init__.py:530\u001b[0m, in \u001b[0;36mRecognizer._listen\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration, stream)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# reached end of the stream\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ben-g\\code\\friend\\venv\\Lib\\site-packages\\speech_recognition\\__init__.py:191\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ben-g\\code\\friend\\venv\\Lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno -9999] Unanticipated host error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     user_text \u001b[38;5;241m=\u001b[39m \u001b[43mrecognize_speech_from_mic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser: \u001b[39m\u001b[38;5;124m\"\u001b[39m, user_text)\n\u001b[0;32m      4\u001b[0m     response \u001b[38;5;241m=\u001b[39m groq_chatbot_conversation(user_text, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-3.1-8b-instant\u001b[39m\u001b[38;5;124m\"\u001b[39m, history_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst_try.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m, in \u001b[0;36mrecognize_speech_from_mic\u001b[1;34m(recognizer)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrecognize_speech_from_mic\u001b[39m(recognizer):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m sr\u001b[38;5;241m.\u001b[39mMicrophone() \u001b[38;5;28;01mas\u001b[39;00m source:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mListening...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m         r\u001b[38;5;241m.\u001b[39mpause_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ben-g\\code\\friend\\venv\\Lib\\site-packages\\speech_recognition\\__init__.py:181\u001b[0m, in \u001b[0;36mMicrophone.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, traceback):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 181\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ben-g\\code\\friend\\venv\\Lib\\site-packages\\speech_recognition\\__init__.py:196\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;66;03m# sometimes, if the stream isn't stopped, closing the stream throws an exception\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_stopped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    197\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpyaudio_stream\u001b[38;5;241m.\u001b[39mstop_stream()\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ben-g\\code\\friend\\venv\\Lib\\site-packages\\pyaudio\\__init__.py:515\u001b[0m, in \u001b[0;36mPyAudio.Stream.is_stopped\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_stopped\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    511\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns whether the stream is stopped.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03m    :rtype: bool\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_stream_stopped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno -9988] Stream closed"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_text = recognize_speech_from_mic(r)\n",
    "    print(\"user: \", user_text)\n",
    "    response = groq_chatbot_conversation(user_text, model=\"llama-3.1-8b-instant\", history_file=\"First_try.txt\")\n",
    "    print(\"bot: \", response)\n",
    "    await tts_play(response, voice=\"en-US-AnaNeural\", rate=\"+50%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
